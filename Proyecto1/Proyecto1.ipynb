{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOrFaYqObW40"
      },
      "source": [
        "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCD02f6EbW44"
      },
      "source": [
        "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
        "\n",
        "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesor: Ignacio Meza, Gabriel Iturra\n",
        "- Auxiliar: Sebasti√°n Tinoco\n",
        "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
        "\n",
        "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
        "\n",
        "### Equipo:\n",
        "\n",
        "- Isidora Luck\n",
        "- \\<Segundo integrante\\>\n",
        "\n",
        "\n",
        "### Link de repositorio de GitHub: `https://github.com/IsidoraLuck/MDS7202.git`\n",
        "\n",
        "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9WiCTIybW46"
      },
      "source": [
        "----\n",
        "\n",
        "## Reglas\n",
        "\n",
        "- **Grupos de 2 personas.**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Estrictamente prohibida la copia.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsPYeMskbW46"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
        "</div>\n",
        "\n",
        "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
        "\n",
        "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
        "- Caracterizaci√≥n autom√°tica de los datos\n",
        "- La soluci√≥n debe ser compatible con cualquier dataset\n",
        "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiHSU6h9bW47"
      },
      "source": [
        "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
        "\n",
        "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
        "\n",
        "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
        "\n",
        "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Reportar el tipo de variable\n",
        "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
        "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
        "    - Si la variables es num√©rica:\n",
        "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
        "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
        "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
        "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
        "\n",
        "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/plots`\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Para las variables num√©ricas:\n",
        "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
        "        - Grafique la correlaci√≥n entre las variables\n",
        "    - Para las variables categ√≥ricas:\n",
        "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
        "        - Grafique el coeficiente V de Cramer entre las variables\n",
        "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
        "    \n",
        "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/clean_data`\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Drop de valores duplicados\n",
        "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
        "        - Drop de valores nulos\n",
        "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
        "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
        "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
        "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
        "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
        "    - Deber√≠an usar `FunctionTransformer`.\n",
        "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
        "\n",
        "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/scale`\n",
        "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
        "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
        "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
        "        - Asuma que no existen datos ordinales en su dataset\n",
        "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
        "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
        "\n",
        "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/clusters`\n",
        "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
        "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
        "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering.\n",
        "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
        "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
        "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
        "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
        "\n",
        "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "\n",
        "    - Crear la carpeta `EDA_fecha/anomalies`\n",
        "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
        "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
        "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a.\n",
        "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
        "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
        "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
        "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
        "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
        "\n",
        "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
        "\n",
        "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
        "\n",
        "Algunas consideraciones generales:\n",
        "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset.\n",
        "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
        "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
        "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.metrics import silhouette_score\n"
      ],
      "metadata": {
        "id": "6cRt4ha1CS_G"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Profiler:\n",
        "\n",
        "    def __init__(self, data, save_path='EDA'):\n",
        "        \"\"\"\n",
        "        Constructor de la clase Profiler.\n",
        "\n",
        "        Parameters:\n",
        "        data (Pandas DataFrame): Los datos a procesar en formato DataFrame.\n",
        "        save_path (str): Ruta donde se guardar√°n los resultados del an√°lisis exploratorio de datos.\n",
        "\n",
        "        Attributes:\n",
        "        data (Pandas DataFrame): Almacena los datos proporcionados.\n",
        "        timestamp (str): La fecha actual en formato \"DD-MM-YYYY\".\n",
        "        eda_folder (str): La carpeta donde se guardar√°n los resultados del an√°lisis exploratorio de datos.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.save_path = save_path\n",
        "        self.timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y\")\n",
        "        self.eda_folder = os.path.join(self.save_path, f\"EDA_{self.timestamp}\")\n",
        "        os.makedirs(self.eda_folder, exist_ok=True)\n",
        "    def summarize(self, variables_of_interest=None, anomalies_threshold=0.05):\n",
        "        \"\"\"\n",
        "        Caracteriza las variables del Dataset.\n",
        "\n",
        "        Parameters:\n",
        "        variables_of_interest (list): Lista de variables a analizar. Si es None, se analizan todas las variables.\n",
        "        anomalies_threshold (float): Umbral para detectar anomal√≠as. Si se supera, se genera una alerta.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.eda_folder):\n",
        "            os.makedirs(self.eda_folder)\n",
        "\n",
        "        if variables_of_interest is None:\n",
        "            variables_of_interest = self.data.columns\n",
        "\n",
        "        summary_file = open(os.path.join(self.eda_folder, \"summary.txt\"), \"w\")\n",
        "\n",
        "        for variable in variables_of_interest:\n",
        "            summary_file.write(f\"Variable: {variable}\\n\")\n",
        "            summary_file.write(f\"Tipo: {self.data[variable].dtype}\\n\")\n",
        "            summary_file.write(f\"Valores √∫nicos: {self.data[variable].nunique()} ({100 * self.data[variable].nunique() / len(self.data):.2f}%)\\n\")\n",
        "            summary_file.write(f\"Valores nulos: {self.data[variable].isnull().sum()} ({100 * self.data[variable].isnull().sum() / len(self.data):.2f}%)\\n\")\n",
        "\n",
        "            if self.data[variable].dtype in ['int64', 'float64']:\n",
        "                summary_file.write(f\"Valores cero: {len(self.data[self.data[variable] == 0])} ({100 * len(self.data[self.data[variable] == 0]) / len(self.data):.2f}%)\\n\")\n",
        "                summary_file.write(f\"Valores negativos: {len(self.data[self.data[variable] < 0])} ({100 * len(self.data[self.data[variable] < 0]) / len(self.data):.2f}%)\\n\")\n",
        "                summary_file.write(f\"Estad√≠sticas descriptivas:\\n{self.data[variable].describe()}\\n\")\n",
        "\n",
        "                if self.data[variable].nunique() > 2:\n",
        "                    z_score = (self.data[variable] - self.data[variable].mean()) / self.data[variable].std()\n",
        "                    outliers = len(z_score[np.abs(z_score) > 3])\n",
        "                    summary_file.write(f\"Valores at√≠picos (z-score > 3): {outliers} ({100 * outliers / len(self.data):.2f}%)\\n\")\n",
        "                    if outliers / len(self.data) > anomalies_threshold:\n",
        "                        summary_file.write(f\"¬°Alerta! M√°s del {100 * anomalies_threshold:.2f}% de valores at√≠picos\\n\")\n",
        "            summary_file.write(\"\\n\")\n",
        "\n",
        "        summary_file.close()\n",
        "\n",
        "\n",
        "    def plot_vars(self, variables_of_interest=None, n_top_categories=10):\n",
        "        \"\"\"\n",
        "        Grafica la distribuci√≥n e interacciones de las variables del Dataset.\n",
        "\n",
        "        Parameters:\n",
        "        variables_of_interest (list): Lista de variables a analizar. Si es None, se analizan todas las variables.\n",
        "        n_top_categories (int): N√∫mero de categor√≠as a mostrar en los histogramas de variables categ√≥ricas.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.eda_folder):\n",
        "            os.makedirs(self.eda_folder)\n",
        "\n",
        "        if variables_of_interest is None:\n",
        "            variables_of_interest = self.data.columns\n",
        "\n",
        "        plots_folder = os.path.join(self.eda_folder, \"plots\")\n",
        "        os.makedirs(plots_folder, exist_ok=True)\n",
        "\n",
        "        for variable in variables_of_interest:\n",
        "            if self.data[variable].dtype in ['int64', 'float64']:\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                sns.histplot(self.data[variable], kde=True)\n",
        "                plt.title(f\"Distribuci√≥n de {variable}\")\n",
        "                plt.xlabel(variable)\n",
        "                plt.ylabel(\"Frecuencia\")\n",
        "                plt.savefig(os.path.join(plots_folder, f\"{variable}.pdf\"))\n",
        "                plt.close()\n",
        "            elif self.data[variable].dtype == 'object':\n",
        "                top_n = self.data[variable].value_counts().head(n_top_categories)\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                top_n.plot(kind='bar')\n",
        "                plt.title(f\"Top {n_top_categories} categor√≠as de {variable}\")\n",
        "                plt.xlabel(variable)\n",
        "                plt.ylabel(\"Frecuencia\")\n",
        "                plt.savefig(os.path.join(plots_folder, f\"{variable}.pdf\"))\n",
        "                plt.close()\n",
        "                if 'target' in self.data.columns:\n",
        "                  le = LabelEncoder()\n",
        "                  data_encoded = self.data[[variable]].apply(le.fit_transform)\n",
        "                  data_encoded['target'] = self.data['target']\n",
        "                  confusion_matrix = pd.crosstab(data_encoded[variable], data_encoded['target'])\n",
        "                  chi2, _, _, _ = chi2_contingency(confusion_matrix)\n",
        "                  cramers_v = np.sqrt(chi2 / (len(self.data) * min(confusion_matrix.shape) - 1))\n",
        "                  plt.figure(figsize=(8, 6))\n",
        "                  sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
        "                  plt.title(f\"Coeficiente V de Cramer entre {variable} y target: {cramers_v:.2f}\")\n",
        "                  plt.xlabel(\"target\")\n",
        "                  plt.ylabel(variable)\n",
        "                  plt.savefig(os.path.join(plots_folder, f\"{variable}_cramer.pdf\"))\n",
        "                  plt.close()\n",
        "\n",
        "\n",
        "    def clean_data(self, variables_of_interest=None, duplicate_subset=None, impute_strategy='mean', atomic_column_contains=None, default_delimiters=['*', '(', ')', '?', ':', '¬ø']):\n",
        "        \"\"\"\n",
        "        Limpia los datos para su procesamiento posterior.\n",
        "\n",
        "        Parameters:\n",
        "        variables_of_interest (list): Lista de variables a limpiar. Si es None, se limpian todas las variables.\n",
        "        duplicate_subset (list): Lista de columnas para identificar duplicados.\n",
        "        impute_strategy (str): M√©todo de imputaci√≥n para valores nulos ('mean', 'median', 'most_frequent', None).\n",
        "        atomic_column_contains (str): Subcadena que deben contener las columnas no at√≥micas. Se espera que esta cadena sea \"age-height-weight\".\n",
        "        default_delimiters (list): Una lista de delimitadores sugeridos para procesar la subcadena especificada en atomic_column_contains.\n",
        "\n",
        "        Returns:\n",
        "        Pandas DataFrame: Datos procesados.\n",
        "        \"\"\"\n",
        "\n",
        "        data = self.data.copy()\n",
        "\n",
        "        # Crear la carpeta EDA_fecha/clean_data si no existe\n",
        "        clean_data_path = f\"{self.save_path}/clean_data\"\n",
        "        os.makedirs(clean_data_path, exist_ok=True)\n",
        "\n",
        "        # Eliminar duplicados si se especifica\n",
        "        if duplicate_subset is not None:\n",
        "            data.drop_duplicates(subset=duplicate_subset, inplace=True)\n",
        "\n",
        "        # Procesamiento de columnas no at√≥micas\n",
        "        if atomic_column_contains and atomic_column_contains in data.columns:\n",
        "            # Usar una expresi√≥n regular para manejar los diferentes delimitadores\n",
        "            pattern = '|'.join([re.escape(delim) for delim in default_delimiters])\n",
        "            split_data = data[atomic_column_contains].str.split(pattern, expand=True)\n",
        "\n",
        "            # Asegurar que siempre hay tres columnas despu√©s de la divisi√≥n\n",
        "            while split_data.shape[1] < 3:\n",
        "                split_data[split_data.shape[1]] = None\n",
        "\n",
        "            # Nombres de las nuevas columnas\n",
        "            new_columns = ['age', 'height', 'weight']\n",
        "\n",
        "            # Asignar las nuevas columnas al DataFrame\n",
        "            for i, col in enumerate(new_columns):\n",
        "                data[col] = pd.to_numeric(split_data[i], errors='coerce')\n",
        "\n",
        "            # Eliminar la columna original\n",
        "            data.drop(atomic_column_contains, axis=1, inplace=True)\n",
        "\n",
        "        # Imputar valores faltantes\n",
        "        for variable in data.columns:\n",
        "            if impute_strategy and data[variable].isnull().any():\n",
        "                if data[variable].dtype in ['int64', 'float64']:\n",
        "                    imputer = SimpleImputer(strategy=impute_strategy)\n",
        "                    data[variable] = imputer.fit_transform(data[[variable]]).ravel()\n",
        "                elif data[variable].dtype == 'object':\n",
        "                    data = data.dropna(subset=[variable])\n",
        "\n",
        "        # Guardar los datos procesados en formato .csv\n",
        "        clean_data_file = f\"{clean_data_path}/data.csv\"\n",
        "        data.to_csv(clean_data_file, index=False)\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "        return data\n",
        "\n",
        "    def scale(self, numerical_scalers=['log', 'min_max'], categorical_scalers=None):\n",
        "        \"\"\"\n",
        "        Prepara adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo.\n",
        "\n",
        "        Parameters:\n",
        "        numerical_scalers (list): T√©cnicas de escalamiento para atributos num√©ricos ('log', 'min_max').\n",
        "        categorical_scalers (list): T√©cnicas de codificaci√≥n para atributos categ√≥ricos ('label_encoder', 'one_hot').\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.eda_folder):\n",
        "            os.makedirs(self.eda_folder)\n",
        "\n",
        "        scale_folder = os.path.join(self.eda_folder, \"scale\")\n",
        "        os.makedirs(scale_folder, exist_ok=True)\n",
        "\n",
        "        # Actualizaci√≥n de las listas de caracter√≠sticas num√©ricas y categ√≥ricas\n",
        "        numerical_features = self.data.select_dtypes(include=['int64', 'float64']).columns\n",
        "        categorical_features = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        transformers = []\n",
        "        transformed_features = []  # Lista para almacenar los nombres de las nuevas caracter√≠sticas transformadas\n",
        "        if 'log' in numerical_scalers:\n",
        "            for feature in numerical_features:\n",
        "                transformers.append((f'log_{feature}', FunctionTransformer(np.log1p, validate=False), [feature]))\n",
        "                transformed_features.append(f'log_{feature}')\n",
        "        if 'min_max' in numerical_scalers:\n",
        "            for feature in numerical_features:\n",
        "                transformers.append((f'min_max_{feature}', MinMaxScaler(), [feature]))\n",
        "                transformed_features.append(f'min_max_{feature}')\n",
        "\n",
        "        if categorical_scalers is not None:\n",
        "            for scaler in categorical_scalers:\n",
        "                if scaler == 'label_encoder':\n",
        "                    for feature in categorical_features:\n",
        "                        le = LabelEncoder()\n",
        "                        self.data[feature] = le.fit_transform(self.data[feature])\n",
        "                        transformed_features.append(feature)\n",
        "\n",
        "        # Aplicar las transformaciones\n",
        "        ct = ColumnTransformer(transformers, remainder='passthrough')\n",
        "        transformed_data = ct.fit_transform(self.data)\n",
        "\n",
        "        # Crear el DataFrame con las columnas correctas\n",
        "        all_features = transformed_features + [col for col in self.data.columns if col not in numerical_features]\n",
        "        scaled_features = pd.DataFrame(transformed_data, columns=all_features)\n",
        "\n",
        "        scaled_features.to_csv(os.path.join(scale_folder, \"scaled_features.csv\"), index=False)\n",
        "\n",
        "\n",
        "    def make_clusters(self, clustering_algorithm='KMeans', n_clusters_range=range(2, 10)):\n",
        "        \"\"\"\n",
        "        Genera clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n.\n",
        "\n",
        "        Parameters:\n",
        "        clustering_algorithm (str): Algoritmo de clustering ('KMeans', 'otro').\n",
        "        n_clusters_range (range): Rango de n√∫mero de clusters para el estudio del codo.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.eda_folder):\n",
        "            os.makedirs(self.eda_folder)\n",
        "\n",
        "        clusters_folder = os.path.join(self.eda_folder, \"clusters\")\n",
        "        os.makedirs(clusters_folder, exist_ok=True)\n",
        "\n",
        "        numerical_features = self.data.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "        if clustering_algorithm == 'KMeans':\n",
        "            silhouette_scores = []\n",
        "            for n_clusters in n_clusters_range:\n",
        "                kmeans = KMeans(n_clusters=n_clusters)\n",
        "                kmeans.fit(self.data[numerical_features])\n",
        "                labels = kmeans.labels_\n",
        "                silhouette_scores.append(silhouette_score(self.data[numerical_features], labels))\n",
        "\n",
        "            optimal_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]\n",
        "            kmeans = KMeans(n_clusters=optimal_n_clusters)\n",
        "            kmeans.fit(self.data[numerical_features])\n",
        "            self.data['cluster'] = kmeans.labels_\n",
        "            reduced_data = PCA(n_components=2).fit_transform(self.data[numerical_features])\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=self.data['cluster'], cmap='viridis')\n",
        "            plt.title(f'Clusters (KMeans) - Silhouette Score: {np.max(silhouette_scores):.2f}')\n",
        "            plt.savefig(os.path.join(clusters_folder, \"clusters.pdf\"))\n",
        "            plt.close()\n",
        "            self.data.to_csv(os.path.join(clusters_folder, \"data_clusters.csv\"), index=False)\n",
        "\n",
        "    def detect_anomalies(self, anomaly_detection_algorithm='IsolationForest', anomalies_threshold=0.05):\n",
        "        \"\"\"\n",
        "        Detecta anomal√≠as en los datos.\n",
        "\n",
        "        Parameters:\n",
        "        anomaly_detection_algorithm (str): Algoritmo de detecci√≥n de anomal√≠as ('IsolationForest', 'otro').\n",
        "        anomalies_threshold (float): Umbral para detectar anomal√≠as.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.eda_folder):\n",
        "            os.makedirs(self.eda_folder)\n",
        "\n",
        "        anomalies_folder = os.path.join(self.eda_folder, \"anomalies\")\n",
        "        os.makedirs(anomalies_folder, exist_ok=True)\n",
        "\n",
        "        numerical_features = self.data.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "        if anomaly_detection_algorithm == 'IsolationForest':\n",
        "            clf = IsolationForest(contamination=anomalies_threshold)\n",
        "            clf.fit(self.data[numerical_features])\n",
        "            self.data['anomaly'] = clf.predict(self.data[numerical_features])\n",
        "            reduced_data = PCA(n_components=2).fit_transform(self.data[numerical_features])\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=self.data['anomaly'], cmap='viridis')\n",
        "            plt.title(f'Anomalies (Isolation Forest) - Threshold: {anomalies_threshold}')\n",
        "            plt.savefig(os.path.join(anomalies_folder, \"anomalies.pdf\"))\n",
        "            plt.close()\n",
        "            self.data.to_csv(os.path.join(anomalies_folder, \"data_anomalies.csv\"), index=False)\n",
        "\n",
        "    def profile(self, variables_of_interest=None, anomalies_threshold=0.05):\n",
        "        \"\"\"\n",
        "        Ejecuta todos los m√©todos anteriores.\n",
        "\n",
        "        Parameters:\n",
        "        variables_of_interest (list): Lista de variables a analizar. Si es None, se analizan todas las variables.\n",
        "        anomalies_threshold (float): Umbral para detectar anomal√≠as. Si se supera, se genera una alerta.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        self.summarize(variables_of_interest, anomalies_threshold)\n",
        "        self.plot_vars(variables_of_interest)\n",
        "        self.clean_data(atomic_column_contains='age-height-weight')\n",
        "        self.scale()\n",
        "        self.make_clusters()\n",
        "        self.detect_anomalies()\n",
        "\n",
        "    def clearGarbage(self):\n",
        "        \"\"\"\n",
        "        Elimina las carpetas/archivos creados por la clase Profiler.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.eda_folder):\n",
        "            import shutil\n",
        "            shutil.rmtree(self.eda_folder)"
      ],
      "metadata": {
        "id": "OBT9SycA6ysR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLRLUuIG1XqH",
        "outputId": "cc80b94a-7ca1-4461-d378-ea6d781dcb08"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset\n",
        "data = pd.read_parquet('/content/drive/MyDrive/olimpiadas.parquet')"
      ],
      "metadata": {
        "id": "y-Il4eEIVcTC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una instancia de la clase Profiler\n",
        "profiler = Profiler(data, save_path='drive/MyDrive/EDA')\n",
        "\n",
        "# Realizar el perfilado de los datos\n",
        "profiler.profile()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RREyDVr_CNDn",
        "outputId": "5efe82ff-ea85-49d8-e9b5-2408120be1ce"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpiar archivos generados por la clase Profiler\n",
        "#profiler.clearGarbage()"
      ],
      "metadata": {
        "id": "4dG3Eox1BV9K"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JruV1dlbW4_"
      },
      "source": [
        "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
        "\n",
        "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
        "\n",
        "1. Introducci√≥n\n",
        "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
        "\n",
        "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
        "\n",
        "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
        "\n",
        "- Describir la tarea asociada al dataset.\n",
        "- Describir brevemente los datos de entrada que les provee el problema.\n",
        "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
        "\n",
        "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
        "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
        "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
        "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
        "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
        "        - ¬øExisten datos duplicados en el conjunto?\n",
        "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
        "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
        "3. Creaci√≥n de Clusters y Anomal√≠as\n",
        "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
        "    \n",
        "4. An√°lisis de Resultados\n",
        "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
        "5. Conclusi√≥n\n",
        "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPSXHPwDbW5A"
      },
      "source": [
        "# 1.- Introducci√≥n\n",
        "\n",
        "En este proyecto, abordamos al desaf√≠o de analizar y caracterizar de manera autom√°tica los datos generados por los Juegos de Santiago 2023, pues este an√°lisis contribuira a la revitalizaci√≥n del pa√≠s, proporcionando informaci√≥n valiosa sobre el desarrollo y el desempe√±o de los juegos.\n",
        "\n",
        "### 1.1- Descripci√≥n de la tarea\n",
        "La tarea principal es realizar un an√°lisis exploratorio de datos (EDA) para comprender la naturaleza de los datos proporcionados en el conjunto *olimpiadas.parquet*. Este conjunto recopila datos de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os. Nuestra misi√≥n es proporcionar informaci√≥n clave, identificar patrones, relaciones y posibles anomal√≠as en los datos. Posteriormente, se podr√°n realizar clusters con los datos obtenidos.\n",
        "\n",
        "### 1.2- Datos de entrada\n",
        "Contamos con el conjunto de datos *olimpiadas.parquet*, que recopila informaci√≥n sobre diferentes aspectos de los juegos ol√≠mpicos. Las columnas del conjunto de datos incluyen informaci√≥n sobre el ID del atleta, nombre, sexo, equipo, a√±o de los juegos, temporada, ciudad, deporte, evento, medalla, edad, altura y peso. Este an√°lisis se centrar√° en explorar estas variables y extraer informaci√≥n significativa para comprender mejor los juegos.\n",
        "\n",
        "### 1.3- Hip√≥tesis\n",
        "Se espera que la clase Profiler identifique autom√°ticamente la diversidad de deportes y la participaci√≥n de atletas de diversas partes del mundo. Tambien se cree que podemos identificar patrones de evoluci√≥n en el rendimiento a lo largo de los a√±os.\n",
        "\n",
        "La clase Profiler deber√≠a facilitar la identificaci√≥n autom√°tica de variables que podr√≠an influir en el √©xito de un pa√≠s o atleta en los juegos. Se utilizar√° para detectar autom√°ticamente posibles anomal√≠as en los datos. Se espera que identifique resultados inusuales o patrones at√≠picos que podr√≠an requerir atenci√≥n y an√°lisis adicionales.\n",
        "\n",
        "# 2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
        "\n",
        "En esta secci√≥n, llevaremos a cabo un an√°lisis exhaustivo de los datos, respondiendo a preguntas clave y proporcionando observaciones sobre el comportamiento de las variables num√©ricas y categ√≥ricas, la presencia de valores nulos, duplicados, y cualquier patr√≥n visual notable.\n",
        "\n",
        "Primero tenemos que ID solo cuenta con valores unicos, Name cuenta con valores tipo object, teniendo 134.732 (el 49.70%) de valores √∫nicos, lo que indica que hay deportistas que se repiten seg√∫n deporte, y 0 datos nulos, solo encontramos 2 generos tenindo 0 valores nulos,  que hay 1184 valores √∫nicos, teniendo que muchos de los teams cuentan con varios deportistas.\n",
        "\n",
        "NOC cuenta con 230 valores √∫nicos y 0 nulos, tenemos que han participado en 51 juegos distintos, los datos comprenden valores de 35 a√±os distintos siendo el menor el 1896 y el mayor el 2016, sin encontrar valores fuera de rango, en cuanto a temporadas solo hay 2 en que se realizan juegos, los que han sido llevaados en 42 ciudades distintas, se tiene que se ha practicado 66 deportes, en 765 eventos, posteriormente, tenemos que en cuanto a la variable Medal encontramos 3 valores unicos correspondientes al tipo de medalla recibida y se encuentran 231.333 (85.33%) valores nulos, correspondientes a las personas que no han recibido medalla.\n",
        "\n",
        "Finalmente, tenemos la variable age-height-weight la que almacena la edad, altura y peso del alteta, esta variable tiene los valores nan del dataset, pero al ser una columna no atomica no se ve reflejada la falta de datos en el ...\n",
        "\n",
        "Al analizar los graficos se tiene que la mayoria de las temporadas son en verano, las medallas son practicamente equitativas, los ultimos a√±os ha aumentado la participaci√≥n, teniendo que el team con m√°s competidores es Estados Unidos.\n",
        "\n",
        "# 3. Creaci√≥n de Clusters y Anomal√≠as\n",
        "\n",
        "En esta etapa, se aplica clustering y detecci√≥n de anomal√≠as para segmentar y identificar patrones en los datos. Teniendo como resultado optimo la creaci√≥n de 2 clusters, los cuales parecieran estar segmentados. Esto tambien se encuentra asociado al metodo de clusterizaci√≥n usado el que es k-means, pues este captura todos los datos cercanos, teniendo promedios de los clusters, en particular estos clusters buscan que exista la menor cantidad de datos mal agrupados optimizando el silhouette score, lo que cobra sentido al visualizar los resultados de las anomalias los que son aleatorios y distribuidos entre los datos de acuerdo a los clusters identificados.\n",
        "\n",
        "Se eligi√≥ KMeans debido a su simplicidad y eficacia en la identificaci√≥n de estructuras de clusters en datos num√©ricos. KMeans es sensible al n√∫mero de clusters, por lo que se realiza un an√°lisis del metodo del codo utilizando el silhouette score para determinar el n√∫mero √≥ptimo de clusters. Para evaluar su optimizaci√≥n se prueba en un rango de 2 a 10 clusters para evaluar diversas configuraciones. Finalmente se realiza PCA para reducir la dimensionalidad a 2 componentes principales con el fin de visualizar los clusters en un gr√°fico bidimensional.\n",
        "\n",
        "Por otro lado para detectar las anomal√≠as se usa Isolation Forest el que es eficiente en la detecci√≥n de anomal√≠as y es capaz de manejar conjuntos de datos grandes. Este se optimiza mediante el uso de un umbral se establece en 0.05, lo que significa que se espera que alrededor del 5% de los datos sean identificados como anomal√≠as.\n",
        "\n",
        "# 4. An√°lisis de Resultados\n",
        "\n",
        "Como se menciona antes se tiene la creaci√≥n de 2 clusters, los cuales parecieran estar segmentados por una de las caracteristicas al realizar PCA, teniendo que existe un corte entre los dos clusters. Encontrando que principalmente las diferencias son en los a√±os de los juegos en los que participaron los atletas y no en sus caracteristicas etarias y fisicas (altura y peso). Cabe destacar que se optimizaron de manera que se minimiza los datos mal agrupados teniendo que las anomalias presentes son aleatorias y distribuidos entre los datos de acuerdo a los clusters identificados.\n",
        "\n",
        "Considerando que el silouthe score es de 0.62 pareciera que los clusters logran agrupar relativamente bien los datos, de manera que las separaciones son bien definidas.\n",
        "\n",
        "\n",
        "# 5. Conclusi√≥n\n",
        "\n",
        "En este proyecto, se abordado el desaf√≠o de analizar y caracterizar de manera autom√°tica los datos generados por los Juegos de Santiago 2023. El an√°lisis exploratorio de datos (EDA) nos proporcion√≥ una comprensi√≥n profunda de la naturaleza del conjunto de datos, revelando informaci√≥n valiosa sobre la diversidad de deportes, la participaci√≥n de atletas de diversas partes del mundo y patrones de evoluci√≥n en el rendimiento a lo largo de los a√±os. Hemos identificado patrones notables en las variables clave, como la diversidad de deportes, la participaci√≥n global de atletas y la evoluci√≥n del rendimiento a lo largo de los a√±os, participaci√≥n que ha aumentado en los √∫ltimos a√±os.\n",
        "\n",
        "\n",
        "Posteriormente mediante profiler, se aplic√≥ con √©xito el algoritmo KMeans para la creaci√≥n de clusters, optimizando el n√∫mero de clusters mediante el m√©todo del codo y evaluando con el Silhouette Score. Se identificaron 2 clusters bien definidos visualizados a trav√©s de PCA. Tambien se realiz√≥ la detecci√≥n de anomal√≠as utilizando Isolation Forest result√≥ en la identificaci√≥n de valores at√≠picos de manera eficiente, destacando √°reas fuera de los clusters principales.\n",
        "\n",
        "\n",
        "Cabe destacar que la segmentaci√≥n en clusters puede proporcionar informaci√≥n valiosa para observar patrones referntes a las caracter√≠sticas de cada grupo de atletas.Lo que apoyado por la detecci√≥n de anomal√≠as que permite identificar eventos o situaciones inusuales en los datos, podr√≠an ayudar a caracterizar a los atletas.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}